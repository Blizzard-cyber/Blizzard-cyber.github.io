[{"id":"bfead0d4bc485b773913efadd71ce0ad","title":"人工智能复习笔记","content":"第一部分 搜索问题目录\n​\t前言\n​\t人工智能历史\n​\t搜索问题\n​\t\t什么是搜索问题？\n​\t不知情搜索算法（Uninformed Search） \n​\t\t一些重要概念\n​\t\t深度优先搜索（DFS）\n​\t\t广度优先搜索（BFS） \n​\t\t代价敏感搜索（CCS） \n​\t\t代价一致搜索（UCS） \n​\t知情搜索算法（Informed Search） \n​\t\t启发式搜索（Heuristics Search） \n​\t\t贪心搜索（Greedy Search） \n​\t\tA*搜索\n​\t\t图搜索（Graph Search） \n​\t\t例题\n​\t\t实验：A*算法解决八数码问题\n​\t对抗搜索\n​\t\t零和游戏（Zero-sum Games）\n​\t\t极小化极大算法（Minimax Algorithm）\n​\t\tAlpha-Beta剪枝算法（Alpha-Beta Pruning） \n​\t\t例题\n\n人工智能历史\n\n搜索问题什么是搜索问题？​\t\t一个搜索问题包括一个状态空间，一个后续函数（包括动作，成本），一个开始状态和一个目标状态。\n​\t\t状态空间：当前“世界”所处的状态所有可能性的集合，假设整个状态空间是一段视频，那么状态就是视频的某一帧截图，它就是组成\t\t状态空间的一种可能。\n​\t\t后续函数：搜索问题的核心，它决定了代理下一步的动作是什么，也就是通过搜索算法来得出当前状态下如何决策是最符合预期的。\t\t后续函数中的参数必然含有动作和成本等协助判断的重要条件，通过这些条件就可以比对出当前状态下最好的决策。\n​\t\t解决方案：搜索问题的目标，是指将开始状态转换为目标状态的一系列操作（计划）。\n​\t计算状态空间的大小\n将所有可能的环境&#x2F;动作全部相乘。\n\n很好理解，总状态空间就是代理位置、食物数量、敌人位置、代理方向四个因素来决定，而代理位置有120种可能，每一颗食物有存在&#x2F;不存在两个状态，所以是2^30种可能，敌人有两个，一共有12种位置，所以敌人位置就有12^2种可能，代理的朝向有四种，就是4种可能。所以总状态空间的大小就是将它们相乘即可。 \n状态空间图与决策树\n\n搜索树中的每个节点都是状态空间图中的一个完整路径，两者都可以按需构建，还可以互相转化，例如根据状态空间图绘制决策树。\n例：将图示状态空间图转化为决策树\n\n我们引入两个列表：open_list 与 closed_list ，分别表示当前正在搜索的节点和已经搜索完毕的节点。open_list 初始只有起始状态S节点，而 closed_list 初始是空的。\n求解过程：（这里其实使用了广度优先搜索）\n\n\n不知情搜索算法（Uninformed Search）一些重要概念​\t完备：如果有解，一定能找到\n​\t最优：保证能找到代价最小的路径\n​\t时间复杂度：算法中的基本操作的执行次数，为算法的时间复杂度。\n​\t空间复杂度：空间复杂度算的是变量的个数\n深度优先搜索（DFS）从上到下，从左到右\n边缘节点是后进先出的堆栈形式\n\n在有限状态空间中，DFS是完备的\nDFS不是最优的 （它只能找到解，在一定条件下也并不保证代价最小）\n广度优先搜索（BFS）从左到右，从上到下\n边缘节点是先进先出的队列形式\n\n在有限状态空间中，BFS是完备的\n在动作未加权时，BFS是最优的\n补充：迭代深化（Iterative Deepening）是旨在将DFS和BFS的优势结合在一起，既然BFS能够找到最浅的(最优的)解，而DFS可能会先找到较深的解，因此我们可以限制深度，先限制DFS的深度为3去搜索，如果没有找到就用最大深度为4的DFS去找，以此类推......这样就可以找到最浅的解从而节约算力。\n\t为什么？明明多次迭代，为何反而节约算力？这是因为**大量的计算通常在更深层**，我们在浅层多次迭代不会影响太多时间&#x2F;空间复杂度。\n\n \n代价敏感搜索（CCS）如下图，如果使用上述方法，无论是DFS，BFS抑或是迭代深化，都只能得到经过节点最少的路径，但是如果每个动作标明代价（加权）的话，经过最少节点的路径就不一定是最优解了。因此，上述方法并不能在代价敏感搜索当中得到最优解，只能通过进一步计算，算出代价从而选出代价最小的路径来实现。\n \n如上图，一共能找到两条路径：SderfG，SerfG。然而代价却分别是3+2+2+2+2&#x3D;11和9+2+2+2&#x3D;15，反而是经过节点较多的SderfG为最优解！\n代价一致搜索（UCS）优先扩展代价最小的节点\n（类似Dijkstra算法）\n\n它是完备的且最优的\n但缺点是每个方向都可能扩展，且不知道目标的确切方向会显得盲目（不知情搜索&#x2F;Uninformed Search是这样的）\n*【解决方法：将UCS和Greedy（贪心算法）结合起来形成A&#x2F;算法…下文会讲到】\n\n知情搜索算法（Informed Search）\n如上图，知情搜索算法要讲解的分别为：启发式搜索，贪心搜索，A-star搜索，图搜索。\n启发式搜索（Heuristics Search）一个启发式是：\n\n▪一个函数，估计一个状态与目标的距离\n\n▪为一个特定的搜索问题设计\n\n▪例子：曼哈顿距离，欧氏距离\n\n\n如图，10+5就是曼哈顿距离：横坐标之差与纵坐标之差的和；11.2是欧氏距离：直线距离。\n这样的函数就叫作启发式函数，记作h(x)。 \n贪心搜索（Greedy Search）**优先扩展当前状态下最优的节点 **\n\n与UCS不同，UCS只考虑当前代价，而Greedy考虑了距离终点的距离。\n它不是最佳的！通常它只考虑了距离而非代价，贪心算法的优劣很大程度上取决于启发式函数h(x)。 \nA*搜索A*搜索综合了UCS和Greedy，有两个评估函数g(x)与h(x)，而A&#x2F;*搜索的决策取决于f(x)&#x3D;g(x)+h(x)，综合考虑了前驱因素与后继因素。\n\nA*是完备且最优的。当f(x)&#x3D;g(x)+0时就找到了最优解。\n【但这里有一个大前提】：那就是估计节点到终点的距离时（h(x)）一定要小于等于实际中s点到终点的距离才行，这叫做可采纳的试探（Admissible Heuristics）\n\n 例如下图：\n\n因此，提出一个可接受的启发式方法是A*算法的需要解决的重要内容（实际上上文提到过的曼哈顿距离和欧氏距离就是很好的启发式方法）。 \n总结：\n\nA*用前驱和（估计的）后继成本来决策；\n\nA*在可接受的启发式方法下是最优的；\n\n启发式设计是关键所在，常用于宽松问题（Relaxed Problems）\n\n\n图搜索（Graph Search）一种在图中寻找路径的方法。\no 图中每个节点对应状态空间中的一个状态 ;o 每条连线对应一个操作符 ( 多数包含它的代价 ) 。** \n 常用开关列表来解决问题：\n1) 建立一个只含有 起始节点 S 的 搜索图 G ， 把 S 放到一个叫做 OPEN 的未扩展节点表中\n2) 建立一个叫做 CLOSED 的已扩展节点表， 其 初始为空表\n3)LOOP ：若 OPEN 表是空表，则失败退出\n4) 选择 OPEN 表上的第一个节点 , 把它从 OPEN 表移出并放进 CLOSED 表中。称此节点为 节点 n\n5) 若 n 为 目标节点， 则有解并成功退出，此解是追踪图 G 中沿着指针从 n 到 S 这条路径而得到的 ( 指针将在第 7 步中设置 )\n6) 扩展 节点 n ，同时 生成不是 n 的祖先的那些后继节点 的 集合 M 。把 M 的这些成员作为 n 的后继节点添入 图 G 中\n7) 生成并记入 M 的子节点有以下三种情况：\n① 未曾在G中出现过 的每个M成员：设置一个通向n的指针，并把它们加进OPEN表\n② 已经在OPEN表上 的每一个M成员，确定是否需更改通到n的指针方向。\n③ 已在CLOSED表上 的每一个M成员：除需确定该子节点指向父节点的指针外，还需确定其后继节点指向 父节点 的指针(也就是，如该子节点的父节点根据需要(如代价值等)改变了，那就把该子节点 移回Open表 )\n8) 按某一任意方式或按某个探试值， 重排 OPEN 表\n9)GO LOOP\n\n** **\n例题用要求的五种搜索算法计算从A到E的最优路径\n 解：\n\n\n实验：A*算法解决八数码问题八数码问题是在 3×3 的九宫格棋盘上，摆有 8 个刻有 1 ～ 8 数码的将牌。棋盘中有一个空格，允许紧邻空格的某一将牌可以移到空格中，这样通过平移将牌可以将某一将牌布局变换为另一布局。针对给定的一种初始布局或结构（目标状态），问如何移动将牌，实现从初始状态到目标状态的转变。如下图表示了一个具体的八数码问题求解。\n解：\n\n选用已经走过的步数为g(n)，选择当前状态与目标状态相异的数字的曼哈顿距离之和为h(n)，得到**估价函数f(n)&#x3D;g(n)+h(n)**。\n选用open表与close表来维护状态图，如果当前状态已经在开&#x2F;关列表中，取最小的一个保留。每次都选当前f(n)最小的作为待搜索节点。\n判断无解情况：将当前状态与目标状态的九宫格转化为线性，分别计算逆序数的个数，如果逆序数奇偶性一致则有解，反之则无解。\n\n\n对抗搜索零和游戏（Zero-sum Games）\n代理有着相反的目标\n\n一个代理的分数高，另一个的分数就会必然变低\n\n\n极小化极大算法（Minimax Algorithm）Minimax算法又名极小化极大算法，是一种找出失败的最大可能性中的最小值的算法。Minimax算法常用于棋类等由两方较量的游戏和程序，这类程序由两个游戏者轮流，每次执行一个步骤。我们众所周知的五子棋、象棋等都属于这类程序，所以说Minimax算法是基于搜索的博弈算法的基础。该算法是一种零总和算法，即一方要在可选的选项中选择将其优势最大化的选择，而另一方则选择令对手优势最小化的方法。\nMinimax是一种悲观算法，即假设对手每一步都会将我方引入从当前看理论上价值最小的格局方向，即对手具有完美决策能力。因此我方的策略应该是选择那些对方所能达到的让我方最差情况中最好的，也就是让对方在完美决策下所对我造成的损失最小。\n例子：\n现在考虑这样一个游戏：有三个盘子A、B和C，每个盘子分别放有三张纸币。A放的是1、20、50；B放的是5、10、100；C放的是1、5、20。单位均为“元”。有甲、乙两人，两人均对三个盘子和上面放置的纸币有可以任意查看。游戏分三步：\n\n甲从三个盘子中选取一个。\n乙从甲选取的盘子中拿出两张纸币交给甲。\n甲从乙所给的两张纸币中选取一张，拿走。\n\n其中甲的目标是最后拿到的纸币面值尽量大，乙的目标是让甲最后拿到的纸币面值尽量小。\n解：由于示例问题格局数非常少，我们可以给出完整的格局树。这种情况下我可以找到Minimax算法的全局最优解。而真实情况中，格局树非常庞大，即使是计算机也不可能给出完整的树，因此我们往往只搜索一定深度，这时只能找到局部最优解。\n正方形表示甲决策（挑选最大），三角形表示乙决策（挑选最小）：\n最终甲期望得到的纸币面值为20，所以他应当选1号盘子为最优决策。 总结一下Minimax算法的步骤：\n\n首先确定最大搜索深度D，D可能达到终局，也可能是一个中间格局。\n在最大深度为D的格局树叶子节点上，使用预定义的价值评价函数对叶子节点价值进行评价。\n自底向上为非叶子节点赋值。其中max节点取子节点最大值，min节点取子节点最小值。\n每次轮到我方时（此时必处在格局树的某个max节点），选择价值等于此max节点价值的那个子节点路径。\n\n注意：\n\n真实问题一般无法构造出完整的格局树，所以需要确定一个最大深度D，每次最多从当前格局向下计算D层。\n因为上述原因，Minimax一般是寻找一个局部最优解而不是全局最优解，搜索深度越大越可能找到更好的解，但计算耗时会呈指数级膨胀。\n也是因为无法一次构造出完整的格局树，所以真实问题中Minimax一般是边对弈边计算局部格局树，而不是只计算一次，但已计算的中间结果可以缓存。\n\nAlpha-Beta剪枝算法（Alpha-Beta Pruning）剪枝算法规则：\n1. Max层的α &#x3D; max(α， 它的所有子结点的评价值)，Max层的β &#x3D; 它的父结点的β2. Min层的β &#x3D; min(β， 它的所有子结点的评价值)，Min层的 α &#x3D; 它的父结点的α3. 当某个结点的 α &gt;&#x3D; β，停止搜索该节点的其他子结点4. 叶结点没有 α 和 β \n它极大减小了时间复杂度，在深层的搜索中减少了计算的负担。\n我们一一说明这四条性质：首先我们假设所有非叶结点的α初始化为负无穷，β初始化为正无穷。\n\n若Max层中发现有一个子结点的评价值比当前所能达到的评价值更大，换句话说就是子结点的操作更优，那么将当前所能达到的评价值换成该子节点的评价值。并且由于它的父结点是从该Max层中选择最小的评价值，那么他就要判断一下当前的α是否大于它父结点的β。为了方便起见，我们将父结点的β赋给它自己的β，这样我们只需要比较它自己的α和β就可以了。\n跟第一条类似，如果发现子结点中有比当前更优的操作（对对手更优，即对自己更差），那么就替换β，同时比较父结点最优解与当前解的大小，如果父结点已经有一个更优解，则不必继续搜索了。\nMax层中，若某个结点的最优解已经大于它的父结点的最差解，则不必继续搜索，剪枝；Min层中，若某个结点的最差解已经小于它的父结点的最优解，则不必继续搜索，剪枝。\n由于叶结点没有子结点，自然不需要计算 α 和 β。\n\n例题\n解：\n\n\n第二部分 MDP和强化学习目录\n​\t前言\n​\t期望最大搜索（Expectimax Search）\n​\t⭐马尔科夫决策（MDP）——offline（超重点）\n​\t\t先来看一个例子\n​\t\t基本概念 \n​\t\t\t政策（Policy）\n​\t\t\t折扣（Discounting）\n​\t\t\t如何停止循环？\n​\t\t\t价值迭代（Value Iteration） \n​\t\t\t例题 \n​\t\t固定策略（Fixed Policies）\n​\t\t策略提取（Policy Extraction）\n​\t\t策略迭代（Policy Iteration）\n​\t\t策略迭代和价值迭代的比较\n​\t强化学习（Reinforcement Learning, RL）——online\n​\t\t简介\n​\t\t基于模型的强化学习（Model-Based RL，MBRL）\n​\t\t无模型强化学习（Model-Free RL，MFRL）\n​\t\t\t直接评估(Direct Evaluation)\n​\t\t\t时间差分学习(Temporal Difference Learning)\n​\t\t主动强化学习（Active Reinforcement Learning）\n​\t\t\tQ-Learning \n​\t\t\t探索与利用 \n\n前言在上一节中，我们提到了Minimax是一种悲观算法，即考虑最坏的情况（Worst Case）从而使损失最小化。然而在实际操作过程中，对手并不是始终能做到最优决策，会有一定概率的失误，因此我们应当计算平均能得到的分数。\n\n当不确定的结果会偶然出现时，也就是在不确定性搜索（Non-Deterministic Search）下，我们的算法就需要做出调整。\n\n期望最大搜索（Expectimax Search）在前言的条件下，对手不一定足够smart去得到最优解，因此，我们将对手节点视作chance nodes，它具有一定的概率去实行一定的策略，此时的策略是使得expected utility最大。值现在应该反映平均情况（预期）结果，而不是最坏情况（最小）结果。上一节提到的minimax实际上是expected max的一种概率为1或0的特例。\n▪期望搜索：计算平均分数下最优玩法\n\n▪最大节点和Minimax一致\n\n▪机会节点类似Minimax的最小节点但结果不确定\n\n▪计算他们的预期效用\n\n▪即加权平均（期望）子节点\n\n注意：在expectimax中最好不要进行剪枝操作，因为min层的计算需要依据下一层的每一个值（如果概率不是非0即1那种）\n\n选择minimax策略的agent总是过于悲观，因此分数不会太高，但胜率会很高；而选择expectimax策略的agent过于乐观（比如万一有一种情况分数很高但概率相对不高，在计算的结果中，导致此算出来的期望值很高，agent会选择这种策略，但事实上，opponent很有可能选择其他路并且令agent分数减少）\n\n\n\n\n⭐马尔科夫决策（MDP）——offline（超重点）MDP是一个五元组&lt;S，A，T，R，γ&gt;——状态空间、行为、状态转移概率、奖励、折扣因子\n先来看一个例子\n只有一个主体，存在障碍，惩罚出口和奖励出口，主体可以任意移动但是有概率出现偏差移动，如果移动碰到墙体则呆在原地，每行动一步会有小的存活奖励（正&#x2F;负&#x2F;0都可）.。\n我们的目标是使主体得到的分数最大化。 \n如下图，以前的决策是左边的情况，而现在要解决的是随机问题\n\n基本概念这就要用到马尔科夫决策过程（MDP）：\nMDP定义为：\n\n▪一组状态集S\n\n▪一组动作集A\n\n▪一个过渡函数T（s,a,s&#39;）\n\n        ▪从状态s到状态s&#39;的概率，例如，P（s&#39;|s, a）\n\n        ▪也称为模型或动态\n\n▪奖励函数R（s,a,s&#39;）\n\n        ▪有时只是R (s)或R (s&#39;)\n\n▪一个起始状态\n\n▪也许存在结束状态\n\n▪马尔科夫决策过程，“马尔科夫”意味着行动结果只取决于当前状态\n\n▪这就类似搜索，后继函数只能取决于当前状态（而非历史状态）\n\n▪MDPs是**非确定性搜索问题**\n\n▪解决它们的一种方法是**期望最大搜索\n\n政策（Policy）在确定性单代理搜索问题，我们想要一个从起始节点到目标节点的最优计划，或序列的行动。\n在MDP中，我们需要一个最优政策，它在每一个状态下都给出一个动作，并且尝试在最后得到最大的效益，显式的策略定义了主体的反应倾向，如下图：\n\n能观察到，在不同的生存奖励下，主体的行动倾向都有所不同。 \n折扣（Discounting）如上图，当生存奖励的负分偏小时，在更为危险的地块中agent会宁愿选择一直对墙试错从而让自己滑行到两侧而非冒险按正确的朝向走，这可能会与我们的实际预期不符，因为它走做了太多无用的动作。这时我们就要给奖励添加折扣，让agent尽可能快的拿到最大的奖励：\n▪最大化奖励的总和是合理的\n\n▪更喜欢马上获得的奖励而非以后的奖励也是合理的\n\n▪一个解决方案：奖励的值呈指数衰减\n\n\n例如，折扣为0.5时，U([1,2,3]) &lt; U([3,2,1])。\n**U([1,2,3]) &#x3D; 1&#x2F;*1 + 0.5&#x2F;*2 + 0.25&#x2F;*3；U([3,2,1]) &#x3D; 1&#x2F;*3 + 0.5&#x2F;*2 + 0.25&#x2F;*1  **\n例如：\n\n在状态d时，γ为多少时往左或右的收益一致？\n解： ，解得γ＝ 。\n如何停止循环？如果一个游戏可以一直进行，怎么让它停下来并呈现出我们的分数？\n\n可以设置在进行n步之后必须结束游戏（life&#x2F;生命周期）\n\n可以设置动态变化的政策，例如随着可用步数的减少，政策随之变化\n\n可以设置折扣，到最后奖励值会趋于收敛，当分数变化小于某个临界时可以结束游戏 \n\n可以设置一个“吸收节点”，当进入这个节点时必须退出游戏，这个节点在前面的阶段不会进入，但到后面终将有可能进入这个状态。\n\n\n价值迭代（Value Iteration）\n起始价值和为0，因为还没有开始迭代\n给定某一向量的价值，开始向后迭代\n\n\n\n重复迭代直至收敛\n\n \n值迭代缺点：\n\n速度慢——每次迭代时间复杂度 O(S²A)\n每个状态的“最大值”很少改变\npolicy通常早在values之前收敛\n\n \n举例：汽车运行问题\n\n\n例题\n\n\n固定策略（Fixed Policies）固定每一步的action由函数π(s)得到，那么V值计算如下，其实和价值迭代没太大区别\n\n\n策略提取（Policy Extraction）在知道每一步的最优价值V&#x2F;*(s)时，还需要进行一个arg max()操作来求得执行哪个action会得到此最优价值\n\n \n\n\n策略迭代（Policy Iteration）包括两部分：\n 策略评估：对于固定策略π ，通过策略评估得到V值，迭代直至v值收敛\n\n策略提升：对于固定策略的V值，使用策略提取获得更好的策略：\n\n局限：在不知道T和R时无法更新V\nidea：对结果 s’（通过做动作！）和平均值进行采样\n\n\n策略迭代和价值迭代的比较两者本质上都是计算最优value，都是用于解决MDP的动态程序\n价值迭代：\n\n每次迭代都更新value和policy\n不跟踪policy，但在选择最大Q值时会隐式的重新计算他\n\n策略迭代：\n\n使用固定策略进行了几次更新实用程序的传递（每次传递都很快，因为我们只考虑一个动作，而不是所有动作）\nAfter the policy is evaluated, a new policy is chosen（慢如值迭代传递）\n新policy会更优\n\n\n强化学习（Reinforcement Learning, RL）——online简介强化学习与MDP的区别就在于：我们不明确转化函数和奖励函数的具体内容，必须切实地去尝试以后才能得出结论！\n\n所以说，强化学习是一种在线学习方式，只能靠自己试错来得出正确的决策。 \n基于模型的强化学习（Model-Based RL，MBRL）*step1.通过training过程，计算状态转移矩阵T（）和动作reward R（），通过学习得到经验MDP模型\n*step2. 使用价值迭代或策略迭代求解最优values\n过程：\n\n选出所有状态\n\n用模型模拟转移函数\n\n模拟奖励函数并且得出价值\n\n用MDP完成剩余的价值迭代等工作\n\n\n例题:\n\n无模型强化学习（Model-Free RL，MFRL）直接评估(Direct Evaluation)计算当前政策下所有动作的价值, 将观察到的样本值作平均\n\n根据政策做出动作\n每次遇到一种情形, 都把(折扣)奖励加起来\n平均这些样本, 得到直接评估结果\n\n例题:\n\n计算过程：\nA &#x3D; [-10] &#x2F; 1 &#x3D;10\n\nB &#x3D; [(-1-1+10)+(-1-1+10)]&#x2F;2 &#x3D; +8\n\nC &#x3D; [(-1+10)+(-1+10)+(-1+10)+(-1-10)]&#x2F;4 &#x3D; +4\n\nD &#x3D; [10+10+10]&#x2F;3 &#x3D; +10\n\nE &#x3D; [(-1-1+10)+(-1-1-10)]&#x2F;2 &#x3D; -2\n\n优点：简单易理解；不需要计算T、R；最终你那个计算出正确的平均value\n\n缺点：浪费了状态连接的信息，每个状态必须单独学习，会花费较长时间学习\n\n\n\n时间差分学习(Temporal Difference Learning)从每段经验中学习 \n\n每次经过一个转移函数(动作)就更新V(s)\n以至于新的状态将会为更新策略作出更多贡献\n政策固定, 始终作评估\n将当前值提供给任何一个后继者并作平均\n\n例题:\n\n计算过程：\n\n \n主动强化学习（Active Reinforcement Learning）Q-Learning我们可以计算出下一个状态的价值并取最大值,但我们也可以计算Q-state(Q状态)的值, 在我理解, 它属于一个未决策的中间态（更关注当前状态和动作）, 计算出它的值可以帮助我们决策, 并且更加有用。\n\n如果知道转化函数和奖励函数：\n如果不知道：\n取一个实例，作为转化函数与奖励函数的值来迭代。\nQ-Learning的属性\n即使没有按最优方式迭代，Q-Learning也始终能够最终迭代为最优结果（非政策学习） \n\n&#96;&#96;&#96;前提条件：\n\n你必须探索足够的次数\n你必须最终使学习率足够小\n但不要太快减少它\n不管你如何选择行动，要求基本上在限制下内 \n  \n\n\n### 探索与利用 \n\n我们通常利用各种函数来帮助我们得出价值等数值帮助决策行为，但这样也不一定是最优解，需要偶尔去进行探索，但在什么条件下进行探索呢？\n* 有几种方案可以强迫探索\n* 最简单：随机行动（ε-贪婪）\n\n* 每次行动，随机一次（使ε为0到1之间的任意数，每次随机出一个0到1的数与它比较）\n* 比ε小，行动随机\n* 比ε大，行动按当前策略\n* 随机行动的问题？\n\n* 我们最终会探索其他可能性，但必须在学习完成后继续研究\n* 解决方案：随着时间的推移降低ε\n\n----\n\n#                                              第三部门  一阶逻辑\n\n**目录**\n\n​\t[逻辑基础](#%E9%80%BB%E8%BE%91%E5%9F%BA%E7%A1%80)\n\n​\t\t[命题的定义](#%E5%91%BD%E9%A2%98%E7%9A%84%E5%AE%9A%E4%B9%89)\n\n​\t\t[命题的真值](#%E5%91%BD%E9%A2%98%E7%9A%84%E7%9C%9F%E5%80%BC)\n\n​\t\t[原子公式](#%E5%8E%9F%E5%AD%90%E5%85%AC%E5%BC%8F)\n\n​\t[连词和量词](#%E8%BF%9E%E8%AF%8D%E5%92%8C%E9%87%8F%E8%AF%8D)\n\n​\t\t[合式公式的真值表](#%E5%90%88%E5%BC%8F%E5%85%AC%E5%BC%8F%E7%9A%84%E7%9C%9F%E5%80%BC%E8%A1%A8)\n\n​\t[等价关系](#%E7%AD%89%E4%BB%B7%E5%85%B3%E7%B3%BB)\n\n​\t[永真蕴含式](#%E6%B0%B8%E7%9C%9F%E8%95%B4%E5%90%AB%E5%BC%8F)\n\n​\t[置换与合一 ](#%E7%BD%AE%E6%8D%A2%E4%B8%8E%E5%90%88%E4%B8%80%C2%A0)\n\n​\t[消解原理 ](#%E6%B6%88%E8%A7%A3%E5%8E%9F%E7%90%86%C2%A0)\n\n​\t[鲁滨逊归结原理](#%E9%B2%81%E6%BB%A8%E9%80%8A%E5%BD%92%E7%BB%93%E5%8E%9F%E7%90%86)\n\n​\t[总结](#%E6%80%BB%E7%BB%93)\n\n​\t[例题](#%E4%BE%8B%E9%A2%98)\n\n----\n\n## 逻辑基础\n\n### 命题的定义\n\n\n\n\n断言：一个陈述句称为一个断言(assertion)\n命题：具有真假意义的断言\n\n### 命题的真值\n\n* **T：命题的意义为真**\n* **F：命题的意义为假**\n**注意：**\n\n* **一个命题不能同时为真和假**\n* **一个命题可以在某些条件下为真，某些条件下为假**\n\n### 原子公式\n\n原子公式：由谓词符号和若干项组成的谓词演算，是谓词演算基本积木块。项包括 : 常量符号、变量符号、函数符号等。\n定义原子公式为真值或假值就表示了某种语义(semantics)若t1,t2, …, tn是项，p是谓词，则称P(t1,t2,…,tn)为原子谓词公式（原子公式）。无变量的原子公式取值确定，包含变量的原子公式取值不定。\n举例：“机器人（ROBOT）在1号房间（room1）内”\n INROOM（ROBOT，room1）为真 INROOM （ROBOT，room2）为假\n\n\n\n----\n\n## 连词和量词\n\n\n与、合取（conjunction)：用连词∧把几个公式连接起来而构成的公式。合取项是合取式的每个组成部分。例：LIKE(I，MUSIC)∧LIKE(I，PAINTING)（我喜爱音乐和绘画。）\n或、析取（disjunction）：用连词∨把几个公式连接起来而构成的公式。析取项是析取式的每个组成部。例：PLAYS(LILI，BASKETBALL)∨PLAYS(LILI，FOOTBALL)（李力打篮球或踢足球。）\n蕴涵（Implication）:“→”表示“如果—那么”（IF—THEN）关系，其所构成的公式叫做蕴涵。\n非（Not）表示否定，¬、～均可表示5.连词的优先级 ：¬, ∧, ∨ (\\exists, \\forall) , →, ↔\n\n\n### 合式公式的真值表\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;f1168271e37f421d86c1dd4c0673d6d4.png)\n\n----\n\n## 等价关系\n\n等价(Equivalence): 如果两个合式公式，无论如何解释，其真值表都是相同的，那么我们就称此两合式公式是等价的。\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;cf19071cc4904ba4a238553ec6c897f5.png)\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;64c6754725d7485c83083c83e6900acd.png)\n\n注意：（10)说明在一个量化的表达式中的约束变量是一类虚元，它可用任何一个不在表达式中出现过的其他变量符号来代替\n\n----\n\n## **永真蕴含式**\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;67df255ffbbc4096a866b956e4f3b074.png)\n\n----\n\n## **置换与合一** \n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2846863188ed44aab471a08c648369ee.png)\n\n置换定义：置换是形如{t1 &#x2F;x1 ,t2 &#x2F;x2 ,…,tn &#x2F;xn }的有限集合，其中t1 ,t2 ,…,tn是项；x1 ,x2 ,…,xn 是互不相同的变元；ti &#x2F;xi 表示用项ti 替换变元xi 。\n\n要求：ti与xi 不能相同，xi 不能循环地出现在另一个ti 中。\n例如：{a&#x2F;x, c&#x2F;y, f(b)&#x2F;z} 是一个置换， 但 {g(z)&#x2F;x, f(x)&#x2F;z}不是一个置换，原因是x和z之间出现了循环置换现象，若改为{g(a)&#x2F;x, f(x)&#x2F;z}即可**\n设θ&#x3D;{t1 &#x2F;x1 ,t2 &#x2F;x2 ,…,tn &#x2F;xn }是一个置换，F是一个谓词公式， 把公式F中出现的所有xi 换成ti(i&#x3D;1,2,…,n)，得到一个新的公式G， 称G为F在置换θ 下的例示，记作**G&#x3D;Fθ\n\n**置换的合成（了解）**\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;15333f08e279428899d1346534498f67.png)\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;b3546a6cd14a4f75baa5268e714ec503.png)\n\n合一定义：设有公式集F&#x3D;{F1 , F2 ,…,Fn }，若存在一个置换θ，可使 F1θ&#x3D;F2θ&#x3D;…&#x3D;Fnθ， 则称θ是F的一个合一。称F1 ,F2 ,…,Fn是可合一的。\n例：\n设有公式集F&#x3D;{P(x, y, f(y)), P(a, g(x), z)}，则 λ&#x3D;{a&#x2F;x, g(a)&#x2F;y, f(g(a))&#x2F;z} 是它的一个合一。\n一般情况下,一个公式集的合一不惟一。\n最一般合一：设σ是谓词公式集F的一个合一，如果对F的**任意一个合一θ都存在一个置换λ，使得 θ&#x3D; σ· λ，则称σ是一个最一 般(或最简单)合一\n\n\n\n----\n\n## 消解原理 \n\n消解:对谓词演算公式进行分解、化简，消去一些符号，以求得导出子句，又称归结消解原理：    (1)一种用于子句公式集的重要推理规则     (2)子句是由文字的析取组成的公式    (3)一个原子公式、原子公式的否定叫作文字注意： 不含任何文字的子句称为空子句。\n由子句、空子句所构成的集合称为子句集消解过程：消解规则应用于母体子句对，以便产生导出子句\n举例：{ E1∨E2 ,～E2∨E3 }消解导出E1∨E3\n\n\n\n**归结的方法：鲁滨逊归结原理**\n\n----\n\n## 鲁滨逊归结原理\n\n核心：两个子句的**归结式、**\n\n定义1：若P是原子谓词公式，则称P与﹁P为互补文字\n\n定义2：设C1和C2是子句集中的任意两个子句，如果C1中的文字**L1**与C2中的文字**L2互补**，那么可从C1和C2中分别**消去L1和L2**，并将C1和C2中余下的部分**按析取关系**构成一个新的子句C12，则称这一过程为归结，**称C12为C1和C2的归结式，称C1和C2为C12的亲本(父辈)子句**\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2fb5ec93f2914213ac50e57b2373116c.png)\n\n----\n\n## 总结\n\n**9步法求取子句集**\n\n* **(1)消去蕴涵符号**\n* **(2)缩小否定符号的辖域(狄·摩根定律)**\n* **(3)变量标准化(哑元唯一)**\n* **(4)消去存在量词**\n* **(5)化为前束形**\n* **(6)化为合取范式(∧)**\n* **(7)消去全称量词**\n* **(8)消去连词符号(∧)**\n* **(9)更换变量名(同一变量名不出现在一个以上子句)**\n\n----\n\n## 例题\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;5086e93c0461466da855c210f5081277.png)\n解：\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;13be1e80c3f346969ac8f6485955a030.png)![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;fa588c70c1b046a9baf2fdf74c4fd7c1.png)![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;e08faebe22af4d6a97e05ebedbe8b95c.png)![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;9ca57af38ecf48b08c148e7eb3d0c188.png) ![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;cea35cf537364910bc45e03db313afe8.png)\n\n* 由①和⑦，用zhang置换x     ~Pass(zhang, computer)∨~Win(zℎang,prize)   ⑧\n* 由③和⑤，用zhang置换u Pass(zhang, v) ⑨\n* 由⑧和⑨，用computer置换v ~Win(zhang, prize)   ⑩\n* 由⑥和⑩，用zhang置换w ~Lucky（zhang）  （11）\n* 由⑤和11可得空子句NIL，所以结论成立，消解树如上图所示。 \n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;94bb3a04010b4415b706bcaacb571161.png)![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2c8a4612057d45f1bbd22de4508f8ab6.png)\n\n#                                     第四部分  概率与贝叶斯网络\n\n**目录**\n\n​\t[概率](#%E6%A6%82%E7%8E%87)\n\n​\t\t[概率公式](#%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F)\n\n​\t\t\t[贝叶斯公式](#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F)\n\n​\t\t\t[链式条件概率](#%E9%93%BE%E5%BC%8F%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87)\n\n​\t\t[例题](#%E4%BE%8B%E9%A2%98)\n\n​\t\t\t[1. 求联合概率分布&#x2F;边缘概率分布&#x2F;条件概率分布](#1.%20%E6%B1%82%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%2F%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%2F%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83)\n\n​\t\t\t[2. 灵活运用贝叶斯公式 ](#2.%20%E7%81%B5%E6%B4%BB%E8%BF%90%E7%94%A8%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F%C2%A0)\n\n​\t   [概率总结](#%E6%A6%82%E7%8E%87%E6%80%BB%E7%BB%93)\n\n​\t[贝叶斯网络](#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C)\n\n​\t\t[判断独立性](#%E5%88%A4%E6%96%AD%E7%8B%AC%E7%AB%8B%E6%80%A7)\n\n​\t\t\t[两个事件独立的判断](#%E4%B8%A4%E4%B8%AA%E4%BA%8B%E4%BB%B6%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%88%A4%E6%96%AD)\n\n​\t\t\t[条件独立性的判断](#%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B%E6%80%A7%E7%9A%84%E5%88%A4%E6%96%AD)\n\n​\t\t\t[假设条件独立的链式法则](#%E5%81%87%E8%AE%BE%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B%E7%9A%84%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99)\n\n​\t\t[⚠Active &#x2F; Inactive Paths 判断独立性](#Active%20%2F%20Inactive%20Paths%20%E5%88%A4%E6%96%AD%E7%8B%AC%E7%AB%8B%E6%80%A7)\n\n​\t\t[贝叶斯网络中的条件概率](#%C2%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87)\n\n​\t\t[编辑](#%E2%80%8B%E7%BC%96%E8%BE%91)\n\n​\t\t[多重连接和多重消除 Multiple Joins &amp; Multiple Elimination](#%C2%A0%E5%A4%9A%E9%87%8D%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%A4%9A%E9%87%8D%E6%B6%88%E9%99%A4%C2%A0Multiple%20Joins%20%26%C2%A0Multiple%20Elimination)\n\n​\t\t[贝叶斯网络抽样(Bayes&#39; Nets Sampling) ](#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E6%8A%BD%E6%A0%B7%28Bayes&#39;%20rel&#x3D;)\n\n​\t\t\t[采样是什么？——Sampling](#%E9%87%87%E6%A0%B7%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E2%80%94%E2%80%94Sampling)\n\n​\t\t\t[先验抽样        ▪ Prior Sampling](#%E5%85%88%E9%AA%8C%E6%8A%BD%E6%A0%B7%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%E2%96%AA%20Prior%20Sampling)\n\n​\t\t\t[拒绝抽样        ▪ Rejection Sampling](#%E6%8B%92%E7%BB%9D%E6%8A%BD%E6%A0%B7%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%E2%96%AA%20Rejection%20Sampling)\n\n​\t\t\t[似然加权        ▪ Likelihood Weighting](#%E4%BC%BC%E7%84%B6%E5%8A%A0%E6%9D%83%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%E2%96%AA%20Likelihood%20Weighting)\n\n​\t\t\t[吉布斯抽样     ▪ Gibbs Sampling](#%E5%90%89%E5%B8%83%E6%96%AF%E6%8A%BD%E6%A0%B7%C2%A0%20%C2%A0%20%C2%A0%E2%96%AA%20Gibbs%20Sampling)\n\n​\t\t\t[总结](#%E6%80%BB%E7%BB%93)\n\n​\t[⚠作业题](#%C2%A0%E4%BD%9C%E4%B8%9A%E9%A2%98)\n\n----\n\n# 概率\n\n----\n\n## 概率公式\n\n### 贝叶斯公式\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;01118b8772564d31b9d044784df5e29d.png) ![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;d4dac3bcc7aa491db531a75c99fc6194.png)\n\n### 链式条件概率\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;21466b87134947dbb3cd83d7b723996c.png)\n\n----\n\n## 例题\n\n### 1. 求联合概率分布&#x2F;边缘概率分布&#x2F;条件概率分布\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;4b9b6bdc337347f992b5133313b34aae.png)\n\n首先明确，P(W |  dry)是一个概率分布，而不是一个概率值。不能写成 P(W |  dry)&#x3D;....\n\n**①求联合概率分布P(D,W);**\n\n**②求边缘概率分布**P(D);\n\n**③求条件概率分布**P(W | D).\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;f3bd997c54894137b20484dd1f24d4d4.png)\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;e94459c1743c40e68cd1247aa2896118.png)\n![image-20230210202657888](C:\\Users\\tao020704\\AppData\\Roaming\\Typora\\typora-user-images\\image-20230210202657888.png)\n\n### 2. 灵活运用贝叶斯公式 \n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;e3030344bb3640c89e9f014fffaf38b0.png)\n\n----\n\n## 概率总结\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;0075f4cf95fd4876beedc74ac5a822da.png)\n\n----\n\n# 贝叶斯网络\n\n----\n\n## 判断独立性\n\n### 两个事件独立的判断\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;90503170aa4a4774a05730cac8d1c5d6.png)\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;4544381008fd4b998bd5f32b7bb40755.png)\n\n### 条件独立性的判断\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;b9a77fcda35742e4ad3fc80b1a18cd4d.png)\n\n### 假设条件独立的链式法则\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;9a590b2990c74254ba525f0e2238243b.png)\n\n----\n\n## ⚠Active &#x2F; Inactive Paths 判断独立性\n\n要判断X，Y的独立性：\n\n1. 找到X到Y的所有路径paths\n\n2. 如果一个path的所有三元组都是active那么此path就是active\n\n​\t**3. 若存在一个path为active，那么X、Y就是非独立的，反之独立**\n\n简言之：找到一条path的所有三元组都是active那么就非独立；\n若只存在一条路径，那么找到一个inactive的三元组就独立，如果全部active才非独立。\n\n**⭐可以把X、Y理解为两个水池，如果有一根连通水管（path）里的开关全打开了（active）那么二者连通，不独立（independence）；如果就只有一根水管连接，那么只要有一个开关被关闭（inactive）那么就独立。**\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;bcf6f00b3448444289aa594626d1d4f6.png)\n\n【上图阴影表示确定条件，即given。】\n\n**⭐只需要记间接因果（中间条件已知）、已知同因（父节点已知）、未知共果是active（子节点未知），其他三个对立的象限自己就出来了。**\n\n----\n\n## 贝叶斯网络中的条件概率\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;38a8f915d7174aef973083fc87b2f1e9.png)\n\n文字描述： 遍历每一项，分别以它们的父节点为条件，连乘即可。\n\n举例说明比较直观：\n\n## ![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;b31abb713d924b8ba4861380b4419103.png)\n\n----\n\n## 多重连接和多重消除 Multiple Joins &amp; Multiple Elimination\n\n对应乘起来就行了，没什么。\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2af8c3c3c14e487bbfd1f92554494235.png)\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;a30f8e16360048f9b623c3cdd7c94008.png)\n\n----\n\n## 贝叶斯网络抽样(Bayes&#39; Nets Sampling) \n\n贝叶斯网络的抽样大致分为：\n\n* 先验抽样        ▪Prior Sampling\n* 拒绝抽样        ▪Rejection Sampling\n* 加权抽样        ▪Likelihood Weighting\n* 吉布斯抽样     ▪Gibbs Sampling\n\n其目的是进一步加快贝叶斯网络近似的速度。\n\n### 采样是什么？——Sampling\n\n采样，顾名思义就是从特定的概率分布中抽取相应样本点的过程。它可以将复杂的分布简化为离散的样本点、可以用于随机模拟已进行复杂模型的近似求解或推理等。\n\n### 先验抽样        ▪ Prior Sampling\n\n获取样本的方式为祖先抽样，从父节点开始逐渐扩展，类似于贝叶斯网络中的联合分布，下面以一个经典的例题直观感受一下：\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;69066807f05b49019d4e2ffb559ec924.png)\n\n如图，它的核心思想是根据有向图的顺序，先对祖先节点进行采样，只有当某个节点的所有父节点都已完成采样，采对该结点进行采样。以上图为场景，先对Cloudy变量进行采样，然后对Sprinkle和Rain变量进行采样，最后对WetGrass变量采样。根据贝叶斯网络的全概率公式\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;411ba681d5654895928d2ae8bffb2e13.png)\n\n        可以看出祖先采样得到的样本服从贝叶斯网络的联合概率分布。\n        如果只需要对贝叶斯网络中一部分随机变量的边缘分布进行采样，可以用祖先采样对全部随机变量进行采样，然后直接忽视那些不需要的变量的采样值即可。由图可见，如果需要对边缘分布p(Rain)进行采样，先用祖先采样得到全部变量的一个样本，如（Cloudy&#x3D;T，Sprinkler&#x3D;T，Rain&#x3D;T，WetGrass&#x3D;T），然后忽略掉无关变量，直接把这个样本看成是Coludy&#x3D;T即可。 \n大致流程：\n\n1. 设一个事件的样本数为：![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;1a2de8b6677d4a2caa5cbff69b9cdf90.png)\n1. 计算生成样本的概率：![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;1a18d022290e4f198c011f8813652cb2.png)\n1. 套用公式：![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;6c3b0c1ae471497dba2e5e43e6d12bd1.png)\n1. 采样程序是一致的，这样就估计出联合分布的概率。\n\n例题：\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;dc47542a02ec45dfb1089a3b7a84f8a3.png)\n\n从贝叶斯网络中得到的样本：\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;115f092702a540999b5622e7be29b615.png)\n\n需要求P（W）\n\n* 计数：+w——4；-w——1\n* 计算概率：+w——4&#x2F;5&#x3D;0.8；-w——1&#x2F;5&#x3D;0.2\n* （样本越多越接近真实概率）\n\n总结：\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;09c6e5b283754618bd9893e4d392ac3f.png)\n\n### 拒绝抽样        ▪ Rejection Sampling\n\n类似，就不赘述了，它采样的方式就是根据条件取，不符合条件的丢弃：\n\n假设在上例中我们要求C的概率，此时保留所有样本就没有意义了，我们选择对C计数；\n\n假设我们想要Ｐ（C|+s），同样的，我们要统计C结果，但忽略（拒绝）没有+s的样本。\n\n这就是拒绝抽样，它对于条件概率而言也是一致的。\n\n![](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;624d15f8952845baa99cbfa728074208.png)\n\n### 似然加权        ▪ Likelihood Weighting\n\n举一个例子：在有观测变量（Sprikeler &#x3D; T, WetGrass &#x3D; T）时，可以先对Cloudy进行采样，再对Rain进行采样，对于Sprikeler、WetGrass则直接赋观察值，得到下面的一个样本：\n![在这里插入图片描述](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20200818094007844.png?x-oss-process&#x3D;image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hobm5uXw&#x3D;&#x3D;,size_16,color_FFFFFF,t_70#pic_center)\n\n这样得到的样本的重要性权值为：\n\nw 正比于 p(Sprinkler &#x3D; T| Cloudy &#x3D; T)*p(WetGrass &#x3D; T| Sprinkler &#x3D; T,Rain &#x3D; T) &#x3D; 0.1&#x2F;**0.99 &#x3D; 0.099该式子可以理解为：当其他变量取得样本中的取值时(Cloudy &#x3D; T,Rain &#x3D; T)，预测变量取得其确定值的可能性。 \n\n![](https://img-blog.csdnimg.cn/c72cc53db0944f1b9c198f7bc298a7e5.png)\n\n### 吉布斯抽样     ▪ Gibbs Sampling\n\n![](https://img-blog.csdnimg.cn/3aaa57145cf6492c932ec200a4642611.png)\n\n![](https://img-blog.csdnimg.cn/59cec3c4afbc4972bf9ed774eae032d1.png)\n\n### 总结\n\n直接采样：按照拓扑顺序依次对每个变量进行采样。变量值被采样的概率分布依赖于父结点已得到的赋值。\n\n拒绝采样：给定一个易于采样的分布，为一个难于采样的分布生成采样样本。\n\n似然加权（likelihood weighting）只生成与证据e一致的事件，从而避免拒绝采样算法的低效率。\n\nGibbs采样算法：贝叶斯网络的Gibbs采样算法从任意的状态出发，通过(给定马尔可夫覆盖)对一个非证据变量Xi随机采样而生成下一个状态。对Xi的采样条件依赖于Xi的马尔可夫覆盖中的变量的当前值。\n\n![](https://img-blog.csdnimg.cn/1fbe93f0e74e4ef3bd2c1c7946cda6f4.png)\n\n----\n\n## ⚠作业题\n\n![](https://img-blog.csdnimg.cn/5aae110ee0c442c5a409a5517d90092b.png)\n\n![](https://img-blog.csdnimg.cn/6f5165e3f46f462c93da9899ed847d84.png)\n\n![](https://img-blog.csdnimg.cn/ab2801b7b194467593f71bc6b76c3fe8.png)\n\n#                                                 第五部分  机器学习 \n\n**目录**\n\n​\t[监督学习  vs 无监督学习](#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0)\n\n​\t[回归 vs 分类 Regression vs Classification](#%C2%A0%E5%9B%9E%E5%BD%92%20vs%20%E5%88%86%E7%B1%BB%C2%A0Regression%20vs%20Classification)\n\n​\t[训练集 vs 测试集 vs 验证集](#%E8%AE%AD%E7%BB%83%E9%9B%86%20vs%20%E6%B5%8B%E8%AF%95%E9%9B%86%20vs%E9%AA%8C%E8%AF%81%E9%9B%86)\n\n​\t[泛化和过拟合 Generalization &amp; Overfitting](#%E6%B3%9B%E5%8C%96%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%C2%A0Generalization%20%26%20Overfitting)\n\n​\t[线性分类器 Linear Classifiers](#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%C2%A0Linear%20Classifiers)\n\n​\t[激活函数 - 概率决策](#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20-%20%E6%A6%82%E7%8E%87%E5%86%B3%E7%AD%96)\n\n​\t[⚠线性回归 ](#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%C2%A0)\n\n​\t[决策树 Decision Trees](#%C2%A0%E5%86%B3%E7%AD%96%E6%A0%91%C2%A0Decision%20Trees)\n\n​\t\t[决策树构建递归退出条件C](#%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E9%80%92%E5%BD%92%E9%80%80%E5%87%BA%E6%9D%A1%E4%BB%B6C)\n\n​\t\t[信息熵 Entropy](#%E4%BF%A1%E6%81%AF%E7%86%B5%C2%A0Entropy)\n\n​\t\t[信息增益 Information Gain](#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%C2%A0Information%20Gain)\n\n​\t\t[⚠ID3算法实例](#ID3%E7%AE%97%E6%B3%95%E5%AE%9E%E4%BE%8B)\n\n----\n\n## 监督学习  vs 无监督学习\n\n监督学习：输入已知类别的数据样本         分类、回归\n\n无监督学习：输入未知类别的数据样本      聚类\n\n----\n\n## 回归 vs 分类 Regression vs Classification\n\n分类：对输入数据进行离散值标签的预测\n\n回归：预测连续的、具体的数值\n\n**Output： 连续 vs 离散**\n\n分类需要激活函数\n\n![](https://img-blog.csdnimg.cn/b1101104e7fb4a1a818b3f5d8c77bccf.png)\n\n----\n\n## 训练集 vs 测试集 vs 验证集\n\n训练集用于学习参数（例如模型概率）\n\n测试集用于计算模型的准确率\n\n验证集用于调节超参数\n\n----\n\n## 泛化和过拟合 Generalization &amp; Overfitting\n\n在有监督学习中，我们会在训练数据集上建立一个模型，之后会把这个模型用于新的，之前从未见过的数据中，这个过程称为模型的**泛化**\n\n模型在训练集上表现好，在测试集验证集表现差就说明出现了**过拟合问题，**出现这种情况的主要原因是训练数据中存在噪音或者训练数据太少\n\n解决办法：选取合适的停止训练标准；使用验证数据集；获取额外数据进行交叉验证；正则化\nRelative frequency parameters will overfit the training data\n\n相对频率参数会过拟合训练数据\n\n----\n\n## 线性分类器 Linear Classifiers\n\n输入特征**向量** f(x) \n\n权重**向量 ** w\n\n**在二分类中：**\n\n真实标签为 y/*∈&#123;-1，1&#125;，\n\n预测标签为 y ，w和f(x)在同一平面则为正样本，y=1，反之y=-1\n\n![](https://img-blog.csdnimg.cn/e8ae99d5282641f19142e11577e311ea.png)\n\n如果分类正确，不更新w，分类错误则更新 w\n\n**w = w + y/* · f(x)  ** 其中y/* = 1或-1\n\n![](https://img-blog.csdnimg.cn/0bec96ae2f9443688300a7636f2f6cee.png)\n\n**在多分类中：**\n\n输入特征**向量** f(x) \n\n每个类别的权重 **向量 **![](https://img-blog.csdnimg.cn/826e883d86e14159ae5819ae40c18b5a.png)\n\n 预测标签为 y ，取![](https://img-blog.csdnimg.cn/4fd9d89b6c9946219da81029a1ba9a4e.png)最大的一个类别标签\n\n![](https://img-blog.csdnimg.cn/423acbf0b378478bbe241c9ec5221943.png)\n\n如果分类正确，不更新w；分类错误则更新 w，此时需要分别对正确和错误的两个w进行更新\n\n**关键点**：**减小**错分类别的向量点积，**增大**真实类别的向量点积\n\n![](https://img-blog.csdnimg.cn/52f15258c5ff4554a061818fa77533ef.png)          ![](https://img-blog.csdnimg.cn/088b9417f10940de8b02c214c9192b5b.png)\n\n----\n\n## 激活函数 - 概率决策\n\n![](https://img-blog.csdnimg.cn/ce625e6c13bb4f7189697a8e8fba4c39.png)\n\n ![](https://img-blog.csdnimg.cn/99ffd12e3cc24839b6656833a6ff890b.png)\n\n----\n\n## **⚠**线性回归 \n\n![](https://img-blog.csdnimg.cn/ef9f8d68c95d413dbf0aecf685f39b7a.png)\n\n![](https://img-blog.csdnimg.cn/390f7fb55b3043dfb5dffb6a7d1140d2.png)\n\n **L2 loss：所有样本的平方误差和**\n\n![](https://img-blog.csdnimg.cn/7c8b3f5bfab44958a1362cded51da268.png)\n\n**例：**\n\n![](https://img-blog.csdnimg.cn/cf4a5a9a7afd49eaa24f983f009fc07c.png)\n\n​                       ![](https://img-blog.csdnimg.cn/a8196578cf774ec397826dfa5185fc32.png)\n\n​                                ![](https://img-blog.csdnimg.cn/deb97beb695141e889ee846cd33f1f5b.png)\n\n----\n\n## 决策树 Decision Trees\n\n### **决策树构建递归退出条件C**\n\n* 当前样本集D包含的样本属于同一类别C\n* 当前属性集A为空或样本集D中所有样本在所有属性上取值相同（但类别可能不相同）\n* 当前结点包含的样本集![](https://img-blog.csdnimg.cn/fa0e339a3be343618d3719e611b430ea.png)为空\n\n### 信息熵 Entropy\n\n信息熵是度量样本集合纯度的指标\n\n假定当前样本集合D中第k类样本所占比例为pk（k=1，2，...，|y|）则D的信息熵的定义为：\n\n![](https://img-blog.csdnimg.cn/0ab855d41e9742b8ae0999da951c7a22.png)\n\n Ent(D)的取值范围为 [0,log2|y| ]，值越小，纯度越高\n\n计算信息熵时约定：若p=0，则![](https://img-blog.csdnimg.cn/08a3038f5c4f4f48bd017f91d833552c.png)=0\n\n### 信息增益 Information Gain\n\n样本集D的某个离散属性a有V个可能的取值![](https://img-blog.csdnimg.cn/88dbd612a69c49caa2ca0a574f7662a3.png)，用a来对D进行划分则会产生V个分支结点，其中第v个分支结点包含了D中所有在在属性a上取值为![](https://img-blog.csdnimg.cn/1b4db973a51243e2a5ec02b3f2b49141.png)的样本，记为![](https://img-blog.csdnimg.cn/d68f0f67527948ee8157c02b2db3330e.png)。定义用属性a对样本集D进行划分所获得的信息增益为：\n\n![](https://img-blog.csdnimg.cn/96a4a1afb6494965a79dd0a19491f019.png)\n\n一般而言，信息增益越大，意味着使用属性a来进行划分获得的纯度提升越大\n\n**在ID3算法中选择****信息增益大****的属性来划分样本集**\n\n### ⚠ID3算法实例\n\n![](https://img-blog.csdnimg.cn/646089f139e14f4d9cca53bea8faf381.png)\n\n![](https://img-blog.csdnimg.cn/2d6df0c45c114e7984f16a71f112af36.png)\n\n \n\n![](https://img-blog.csdnimg.cn/d6049dc40ddf43498a498eeb690047b0.png)\n\n![](https://img-blog.csdnimg.cn/cb162af2881a41998708d1db0690c476.png)\n\n \n\n![](https://img-blog.csdnimg.cn/c00d9899e3524b7e87549c87b18d4547.png)\n\n\n\n![](https://img-blog.csdnimg.cn/a25cfd917d55499cbc43dfb6c53c6c19.png)\n\n","slug":"about","date":"2023-03-13T15:07:40.000Z","categories_index":"","tags_index":"","author_index":"凡"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","slug":"hello-world","date":"2023-03-12T14:02:35.835Z","categories_index":"","tags_index":"","author_index":"凡"}]